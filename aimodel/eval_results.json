{
  "timestamp": "2026-01-29T03:45:26.787335500+00:00",
  "model_config": {
    "name": "spatialvortex-7b-dev",
    "training_epochs": 100,
    "gpu_enabled": false,
    "features": [
      "sacred_geometry",
      "vortex_cycles",
      "elp_attributes",
      "moe_routing"
    ]
  },
  "task_results": [
    {
      "task": "mmlu",
      "accuracy": 32.0,
      "num_correct": 32,
      "total": 100,
      "avg_confidence": 0.7111136,
      "time_secs": 0.6412495,
      "wrong_samples": []
    },
    {
      "task": "gsm8k",
      "accuracy": 26.0,
      "num_correct": 26,
      "total": 100,
      "avg_confidence": 0.7086483,
      "time_secs": 1.0963731,
      "wrong_samples": []
    },
    {
      "task": "arc",
      "accuracy": 15.0,
      "num_correct": 15,
      "total": 100,
      "avg_confidence": 0.8097775,
      "time_secs": 1.1452836,
      "wrong_samples": []
    },
    {
      "task": "hellaswag",
      "accuracy": 32.0,
      "num_correct": 32,
      "total": 100,
      "avg_confidence": 0.89610296,
      "time_secs": 1.546781,
      "wrong_samples": []
    },
    {
      "task": "truthfulqa",
      "accuracy": 20.0,
      "num_correct": 20,
      "total": 100,
      "avg_confidence": 0.69078964,
      "time_secs": 1.0266297,
      "wrong_samples": []
    },
    {
      "task": "humaneval",
      "accuracy": 84.0,
      "num_correct": 84,
      "total": 100,
      "avg_confidence": 0.9786055,
      "time_secs": 2.1654221,
      "wrong_samples": []
    },
    {
      "task": "commonsenseqa",
      "accuracy": 18.0,
      "num_correct": 18,
      "total": 100,
      "avg_confidence": 0.8222796,
      "time_secs": 1.8553741000000001,
      "wrong_samples": []
    },
    {
      "task": "squad",
      "accuracy": 10.0,
      "num_correct": 10,
      "total": 100,
      "avg_confidence": 0.8671564,
      "time_secs": 1.5362257000000001,
      "wrong_samples": []
    },
    {
      "task": "babi1",
      "accuracy": 84.0,
      "num_correct": 84,
      "total": 100,
      "avg_confidence": 0.9079658,
      "time_secs": 1.4395888000000001,
      "wrong_samples": []
    },
    {
      "task": "babi2",
      "accuracy": 72.0,
      "num_correct": 72,
      "total": 100,
      "avg_confidence": 0.87394875,
      "time_secs": 1.4871602,
      "wrong_samples": []
    },
    {
      "task": "babi3",
      "accuracy": 85.0,
      "num_correct": 85,
      "total": 100,
      "avg_confidence": 0.82391965,
      "time_secs": 1.5800524999999999,
      "wrong_samples": []
    },
    {
      "task": "babi15",
      "accuracy": 78.0,
      "num_correct": 78,
      "total": 100,
      "avg_confidence": 0.9786173,
      "time_secs": 1.4561229,
      "wrong_samples": []
    },
    {
      "task": "babi16",
      "accuracy": 100.0,
      "num_correct": 100,
      "total": 100,
      "avg_confidence": 1.0,
      "time_secs": 2.0842334,
      "wrong_samples": []
    }
  ],
  "overall": {
    "total_correct": 656,
    "total_questions": 1300,
    "overall_accuracy": 50.46153846153846,
    "total_time_secs": 36.1177088
  }
}