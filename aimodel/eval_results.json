{
  "timestamp": "2026-01-29T00:56:00.707409700+00:00",
  "model_config": {
    "name": "spatialvortex-7b-dev",
    "training_epochs": 100,
    "gpu_enabled": false,
    "features": [
      "sacred_geometry",
      "vortex_cycles",
      "elp_attributes",
      "moe_routing"
    ]
  },
  "task_results": [
    {
      "task": "mmlu",
      "accuracy": 27.0,
      "num_correct": 27,
      "total": 100,
      "avg_confidence": 0.6131144,
      "time_secs": 1.3908112,
      "wrong_samples": []
    },
    {
      "task": "gsm8k",
      "accuracy": 27.0,
      "num_correct": 27,
      "total": 100,
      "avg_confidence": 0.6985199,
      "time_secs": 0.9104345,
      "wrong_samples": []
    },
    {
      "task": "arc",
      "accuracy": 25.0,
      "num_correct": 25,
      "total": 100,
      "avg_confidence": 0.6135759,
      "time_secs": 0.69558,
      "wrong_samples": []
    },
    {
      "task": "hellaswag",
      "accuracy": 28.000000000000004,
      "num_correct": 28,
      "total": 100,
      "avg_confidence": 0.7304966,
      "time_secs": 0.8701801,
      "wrong_samples": []
    },
    {
      "task": "truthfulqa",
      "accuracy": 20.0,
      "num_correct": 20,
      "total": 100,
      "avg_confidence": 0.6476791,
      "time_secs": 0.3417871,
      "wrong_samples": []
    },
    {
      "task": "humaneval",
      "accuracy": 70.0,
      "num_correct": 70,
      "total": 100,
      "avg_confidence": 0.7340263,
      "time_secs": 1.2339348,
      "wrong_samples": []
    },
    {
      "task": "commonsenseqa",
      "accuracy": 23.0,
      "num_correct": 23,
      "total": 100,
      "avg_confidence": 0.60539746,
      "time_secs": 0.6357734,
      "wrong_samples": []
    },
    {
      "task": "squad",
      "accuracy": 14.000000000000002,
      "num_correct": 14,
      "total": 100,
      "avg_confidence": 0.6546844,
      "time_secs": 0.4930751,
      "wrong_samples": []
    },
    {
      "task": "babi1",
      "accuracy": 83.0,
      "num_correct": 83,
      "total": 100,
      "avg_confidence": 0.9046998,
      "time_secs": 0.3823241,
      "wrong_samples": []
    },
    {
      "task": "babi2",
      "accuracy": 85.0,
      "num_correct": 85,
      "total": 100,
      "avg_confidence": 0.77799493,
      "time_secs": 0.3896225,
      "wrong_samples": []
    },
    {
      "task": "babi3",
      "accuracy": 86.0,
      "num_correct": 86,
      "total": 100,
      "avg_confidence": 0.79395694,
      "time_secs": 0.4773079,
      "wrong_samples": []
    },
    {
      "task": "babi15",
      "accuracy": 20.0,
      "num_correct": 20,
      "total": 100,
      "avg_confidence": 0.7054634,
      "time_secs": 0.4009346,
      "wrong_samples": []
    },
    {
      "task": "babi16",
      "accuracy": 100.0,
      "num_correct": 100,
      "total": 100,
      "avg_confidence": 1.0,
      "time_secs": 0.9264404,
      "wrong_samples": []
    }
  ],
  "overall": {
    "total_correct": 608,
    "total_questions": 1300,
    "overall_accuracy": 46.76923076923077,
    "total_time_secs": 9.1599528
  }
}