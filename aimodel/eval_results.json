{
  "timestamp": "2026-02-21T02:27:51.081404700+00:00",
  "model_config": {
    "name": "spatialvortex-7b-dev",
    "training_epochs": 0,
    "gpu_enabled": false,
    "features": [
      "sacred_geometry",
      "vortex_cycles",
      "elp_attributes",
      "moe_routing"
    ]
  },
  "task_results": [
    {
      "task": "mmlu",
      "accuracy": 86.66666666666667,
      "num_correct": 39,
      "total": 45,
      "avg_confidence": 0.64521855,
      "time_secs": 0.9635548,
      "wrong_samples": []
    },
    {
      "task": "gsm8k",
      "accuracy": 37.77777777777778,
      "num_correct": 17,
      "total": 45,
      "avg_confidence": 0.6103458,
      "time_secs": 1.7847572999999999,
      "wrong_samples": []
    },
    {
      "task": "arc",
      "accuracy": 84.44444444444444,
      "num_correct": 38,
      "total": 45,
      "avg_confidence": 0.6942368,
      "time_secs": 2.0731921,
      "wrong_samples": []
    },
    {
      "task": "hellaswag",
      "accuracy": 93.33333333333333,
      "num_correct": 42,
      "total": 45,
      "avg_confidence": 0.80329025,
      "time_secs": 4.8502222,
      "wrong_samples": []
    },
    {
      "task": "truthfulqa",
      "accuracy": 97.77777777777777,
      "num_correct": 44,
      "total": 45,
      "avg_confidence": 0.7876103,
      "time_secs": 1.815005,
      "wrong_samples": []
    },
    {
      "task": "humaneval",
      "accuracy": 100.0,
      "num_correct": 45,
      "total": 45,
      "avg_confidence": 0.9935931,
      "time_secs": 6.1836766,
      "wrong_samples": []
    }
  ],
  "overall": {
    "total_correct": 225,
    "total_questions": 270,
    "overall_accuracy": 83.33333333333334,
    "total_time_secs": 21.7059246
  }
}