# âš¡ Quick Start: Multi-Model Chat

## What Changed

You now get **4 AI responses** instead of 1:
1. ðŸ¤– **llama3.2:latest** (gray bubble)
2. ðŸ¤– **mixtral:8x7b** (gray bubble)
3. ðŸ¤– **codellama:13b** (gray bubble)
4. ðŸŒ€ **Vortex** (ORANGE bubble, synthesizes all)

---

## ðŸš€ Start Commands

### Terminal 1
```bash
ollama serve
```

### Terminal 2
```bash
cargo run --release --bin api_server --features agents,persistence,postgres,lake,burn-cuda-backend
```

### Terminal 3
```bash
cd web
bun run dev
```

### Browser
```
http://localhost:28082
```

---

## âœ… What You'll See

Type: **"What is consciousness?"**

You'll get:
- 3 gray bubbles (each Ollama model)
- 1 orange bubble (Vortex synthesis)

First query: ~40-60 seconds (models loading)  
After that: ~10-30 seconds per query

---

## ðŸŽ¯ Success!

If you see 4 separate responses with the last one in **orange** labeled "ðŸŒ€ Vortex", it's working! ðŸŽ‰
