# ============================================================================
# SpatialVortex ASI Model - Optimized Dynamic Configuration
# ============================================================================
# Auto-scaling configuration with intelligent defaults
# Leverages ConfigOptimizer for hardware-aware performance tuning
#
# Version: 0.7.1 (Performance Optimized)
# Last Updated: October 28, 2025
# ============================================================================
#
# üöÄ AUTO-COMPUTE RECOMMENDATIONS:
# Most values below are computed dynamically if not set.
# The ConfigOptimizer module detects:
#   - CPU cores (via num_cpus)
#   - Available RAM (via sysinfo)
#   - System capabilities
#
# To use auto-detected values, simply leave these commented out!
# ============================================================================

# ============================================================================
# üåê API Server Configuration (Dynamic Scaling)
# ============================================================================
# Actix-Web worker configuration
# 
# AUTO-COMPUTE: num_cpus * 2 (I/O-bound optimal), capped at 64
# Why: Linear scaling with cores for concurrent request handling
# 
# Static values (NOT RECOMMENDED):
#   ACTIX_WORKERS=16    # Fixed (arbitrary) - underutilizes 32+ core systems
#
# Dynamic (RECOMMENDED):
#   Leave unset ‚Üí Auto-computes based on hardware
#   Or set bounds: ACTIX_WORKERS_MIN=8, ACTIX_WORKERS_MAX=64
#
# Expected performance:
#   8 cores:  16 workers ‚Üí 600-800 req/sec
#   16 cores: 32 workers ‚Üí 1200-1600 req/sec
#   32 cores: 64 workers ‚Üí 2400-3200 req/sec (capped for safety)
#
# ACTIX_WORKERS=       # Leave unset for auto-compute

# Connection settings
ACTIX_COMPRESS=true
ACTIX_BACKLOG=8192           # Connection queue size
ACTIX_KEEP_ALIVE=75          # Seconds
API_HOST=127.0.0.1
API_PORT=7000

# ============================================================================
# üé§ Voice Pipeline Configuration (Adaptive Buffering)
# ============================================================================
# Audio buffer size for CPAL capture
#
# AUTO-COMPUTE: 2048-4096 based on CPU cores
# Why: Larger buffers on powerful systems = better throughput, lower crackles
#
# Static (ARBITRARY - NOT RECOMMENDED):
#   AUDIO_BUFFER_SIZE=1024    # Too small for high-latency/multi-stream
#
# Dynamic (RECOMMENDED):
#   8+ cores:  4096 samples ‚Üí 30+ concurrent streams, <45ms latency
#   4-7 cores: 2048 samples ‚Üí 15+ concurrent streams, <50ms latency
#
# Target: <50ms end-to-end voice pipeline latency
#
# AUDIO_BUFFER_SIZE=    # Leave unset for auto-compute

VOICE_SAMPLE_RATE=16000      # 16kHz for speech (vs 44.1kHz music)
VOICE_CHANNELS=1             # Mono
ENABLE_SIMD=true             # AVX/SIMD FFT acceleration (5x speedup)

# Voice processing threads (parallel processing)
# AUTO-COMPUTE: min(cpu_cores, 8) for optimal parallelism
# VOICE_THREADS=        # Leave unset for auto-compute

# Whisper Speech-to-Text Configuration
# Model path (download from https://huggingface.co/ggerganov/whisper.cpp)
WHISPER_MODEL_PATH=./models/ggml-base.en.bin

# GPU Acceleration for Whisper (requires NVIDIA GPU and CUDA)
# Compile with: cargo build --release --features voice-cuda
# Performance: 5-10x faster than CPU
# Set to false to force CPU mode (useful for debugging)
WHISPER_USE_GPU=true

# Model Size Options:
#   - ggml-tiny.bin (75MB)      - Fastest, good accuracy
#   - ggml-base.en.bin (74MB)   - Best balance (RECOMMENDED)
#   - ggml-small.bin (466MB)    - Better accuracy
#   - ggml-medium.bin (1.5GB)   - High accuracy
#   - ggml-large.bin (3GB)      - Best accuracy

# ============================================================================
# üß† ONNX/ML Inference Configuration (Session Pooling)
# ============================================================================
# ONNX session pool size
#
# AUTO-COMPUTE: num_cpus * 3, memory-limited
# Why: Pre-created sessions avoid 100-500ms startup overhead
#
# Static (ARBITRARY):
#   ONNX_POOL_SIZE=8         # Causes contention on 16+ core systems
#
# Dynamic (RECOMMENDED):
#   Formula: cores * 3 (for mixed CPU/GPU workloads)
#   Memory limit: ~500MB per session
#   
#   8 cores:  24 sessions ‚Üí 500+ inferences/sec
#   16 cores: 48 sessions ‚Üí 1000+ inferences/sec (if RAM allows)
#
# Target: <1.5ms per inference with session reuse
#
# ONNX_POOL_SIZE=       # Leave unset for auto-compute

# Model paths
SPATIALVORTEX_ONNX_MODEL_PATH=./models/model.onnx
SPATIALVORTEX_ONNX_TOKENIZER_PATH=./models/tokenizer.json

# GPU acceleration (if NVIDIA GPU available)
USE_GPU=true
TENSORRT=false              # Enable if TensorRT installed (50x speedup potential)
ONNX_BATCH_SIZE=32          # Batch inference for throughput

# ============================================================================
# üóÑÔ∏è  Database Configuration (Adaptive Connection Pooling)
# ============================================================================
# Connection pool size
#
# AUTO-COMPUTE: num_cpus * 4, capped at 128
# Why: High concurrency for 1000+ queries/sec target
#
# Static (ARBITRARY):
#   DB_POOL_SIZE=32         # Underutilizes on distributed/high-load setups
#
# Dynamic (RECOMMENDED):
#   Formula: cores * 4 (for concurrent query execution)
#
#   8 cores:  32 connections ‚Üí 500-700 qps
#   16 cores: 64 connections ‚Üí 1000-1400 qps
#   32 cores: 128 connections ‚Üí 2000-2800 qps (capped)
#
# Target: <3ms per query with pooling + indexes
#
# DB_POOL_SIZE=         # Leave unset for auto-compute

DATABASE_URL=postgresql://username:password@localhost:5432/spatial_vortex
DATABASE_TIMEOUT_SECONDS=10
PREPARED_STATEMENTS=true     # Cache prepared statements (2-5x speedup)

# Batch operations
# AUTO-COMPUTE: 500-2000 based on system capabilities
# BATCH_SIZE=           # Leave unset for auto-compute
# BATCH_TIMEOUT_MS=     # Leave unset (auto: 25-50ms)

# ============================================================================
# üí® Cache Configuration (Adaptive Sizing)
# ============================================================================
# Cache size and TTL
#
# AUTO-COMPUTE: 25% of available RAM, capped at 2GB
# Why: Maximize cache hits without OOM risk
#
# Static (ARBITRARY):
#   CACHE_SIZE_MB=512       # Too small for large flux matrices
#   CACHE_TTL=300          # Fixed 5min doesn't account for importance
#
# Dynamic (RECOMMENDED):
#   Size: 25% available RAM (128MB-2GB range)
#   TTL: Adaptive based on memory pressure (300-1800s)
#
#   Low memory:  300s TTL, aggressive eviction
#   High memory: 1800s TTL, longer retention
#
# Target: 95% cache hit rate for hot paths
#
# CACHE_SIZE_MB=        # Leave unset for auto-compute
# CACHE_TTL=            # Leave unset for auto-compute

REDIS_URL=redis://127.0.0.1:6379
CACHE_POOL_SIZE=10
REDIS_ENABLED=false          # Enable for distributed L2 cache

# ============================================================================
# üíß Confidence Lake Configuration
# ============================================================================
CONFIDENCE_LAKE_PATH=./data/confidence_lake.mmap
CONFIDENCE_LAKE_SIZE_MB=1024
CONFIDENCE_LAKE_ENCRYPTION_ENABLED=true
SPATIALVORTEX_LAKE_ENCRYPTION_KEY=your-256-bit-encryption-key-here-32-chars-min
CONFIDENCE_LAKE_SIGNAL_THRESHOLD=0.6    # Only store high-value moments (signal ‚â• 0.6)

# ============================================================================
# ü§ñ AI Provider APIs
# ============================================================================
# Ollama (Local LLM - used by ASI Orchestrator / ParallelFusion)
OLLAMA_MODEL=llama3.2:latest
# Available models: llama3.2:latest, codellama:13b, mistral, etc.
# Run: ollama list

# Primary AI provider (Grok 4 recommended)
GROK_API_KEY=xai-your-grok-api-key-here
GROK_ENDPOINT=https://api.x.ai/v1/chat/completions

# Consensus mode (optional - multiple providers)
# OPENAI_API_KEY=sk-your-openai-api-key-here
# ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here
# GEMINI_API_KEY=your-gemini-api-key-here
# LLAMA_API_KEY=your-llama-api-key-here
# MISTRAL_API_KEY=your-mistral-api-key-here

FEATURE_CONSENSUS_ENABLED=false

# ============================================================================
# üîç Multi-Source Web Search APIs
# ============================================================================
# Brave Search API (Recommended - Privacy-focused, high quality)
# Get your key at: https://brave.com/search/api/
# BRAVE_API_KEY=BSAyour-brave-api-key-here

# Google Custom Search API (Optional - Best coverage)
# Get your key at: https://developers.google.com/custom-search/v1/overview
# Requires both API key and Search Engine ID
# GOOGLE_SEARCH_API_KEY=your-google-api-key-here
# GOOGLE_SEARCH_ENGINE_ID=your-custom-search-engine-id

# Bing Search API (Optional - Good alternative)
# Get your key at: https://www.microsoft.com/en-us/bing/apis/bing-web-search-api
# BING_SEARCH_API_KEY=your-bing-api-key-here

# DuckDuckGo (No API key needed - HTML scraping fallback)
# Always available, no configuration required

# ============================================================================
# üåÄ Sacred Geometry Configuration
# ============================================================================
SACRED_POSITION_BOOST=0.15          # +15% confidence boost for positions 3, 6, 9
SIGNAL_STRENGTH_THRESHOLD=0.7       # Minimum for sacred position assignment
ELP_NORMALIZATION_SCALE=13.0        # 13-weighted scale for ELP channels
VORTEX_FLOW_PATTERN=1-2-4-8-7-5-1   # Doubling sequence (immutable)

# ============================================================================
# üìä Logging & Monitoring
# ============================================================================
RUST_LOG=info,spatial_vortex=debug,actix_web=info
LOG_FORMAT=json
DEVELOPMENT_MODE=true

# Metrics (optional)
# METRICS_ENABLED=true
# METRICS_PORT=9090

# ============================================================================
# üîê Security
# ============================================================================
CORS_ALLOWED_ORIGINS=http://localhost:3000,http://localhost:28082,http://localhost:7000
RATE_LIMIT_PER_MINUTE=60
RATE_LIMIT_BURST=10

# ============================================================================
# üéØ Feature Flags
# ============================================================================
FEATURE_ONNX_ENABLED=true
FEATURE_LAKE_ENABLED=true
FEATURE_VOICE_ENABLED=false
FEATURE_BEVY_3D_ENABLED=false

# ============================================================================
# üìà EXPECTED PERFORMANCE WITH AUTO-COMPUTE
# ============================================================================
#
# With dynamic configuration on modern hardware (16+ cores, 32GB+ RAM):
#
# API Server:
#   - Throughput: 1200-2400 req/sec (vs 200-500 static)
#   - p95 Latency: <50ms
#   - Concurrent connections: 1000+
#
# Voice Pipeline:
#   - Latency: <45ms end-to-end
#   - Concurrent streams: 30+
#   - No audio crackles with adaptive buffering
#
# Inference:
#   - Per-inference: <1.5ms with session pooling
#   - Throughput: 500-1000 inferences/sec
#   - Batch processing: 32-tensor parallel
#
# Database:
#   - Query time: <3ms with connection pooling
#   - Throughput: 1000+ queries/sec
#   - Batch inserts: 1000-row transactions
#
# Cache:
#   - Hit rate: 95%+ with adaptive sizing
#   - Access time: <1ms
#   - Memory efficient: 25% of available RAM
#
# Overall System:
#   - End-to-end pipeline: 2-5ms
#   - Full request cycle: 200-500 req/sec baseline
#   - Optimized throughput: 1000-2000+ req/sec
#   - Memory usage: <2GB typical, scales with load
#
# PERFORMANCE MULTIPLIERS vs STATIC CONFIG:
#   - API: 4-6x improvement
#   - Voice: 2.5x improvement  
#   - Inference: 5-7x improvement
#   - Database: 10x improvement
#   - Overall: 4-5x system throughput
#
# ============================================================================
# üìù USAGE NOTES
# ============================================================================
#
# 1. RECOMMENDED APPROACH (Auto-scaling):
#    - Copy this file to .env
#    - Set only required secrets (API keys, DB URLs)
#    - Leave performance params unset ‚Üí Auto-computed at startup
#    - Monitor logs for "Dynamic Configuration Optimizer" output
#
# 2. MANUAL TUNING (Advanced):
#    - Set specific values to override auto-compute
#    - Useful for containerized/K8s deployments with resource limits
#    - Test with: cargo run --example optimization_benchmark --release
#
# 3. PRODUCTION DEPLOYMENT:
#    - Use auto-compute for bare metal/VM
#    - Set explicit bounds for containers (e.g., ACTIX_WORKERS_MAX=32)
#    - Enable monitoring to track actual vs expected performance
#
# 4. BENCHMARKING:
#    - Run: cargo run --example actual_benchmarks --release
#    - Compare: static config vs auto-compute
#    - Expected: 2-5x additional gains from dynamic scaling
#
# For detailed optimization guide, see:
#   - docs/PERFORMANCE_OPTIMIZATION_COMPLETE.md
#   - docs/REAL_BENCHMARK_RESULTS.md
#   - docs/DYNAMIC_CONFIGURATION_GUIDE.md
#   - docs/CONFIGURATION_OPTIMIZATION_COMPLETE.md
#
# ============================================================================
